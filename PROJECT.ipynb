{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","import os\n","folderpath = r\"/home/muhammdhussain/files1\"\n","filepath = [os.path.join(folderpath,name) for name in os.listdir(folderpath)]\n","Allfiles = []\n","\n","for path in filepath:\n","    with open(path,'r',encoding=\"ISO-8859-1\") as f:\n","        file = f.readlines()\n","        Allfiles.append(file)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","Allfiles\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","print(len(Allfiles))\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","DataInStr = ''.join(map(str,Allfiles))\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","DataInStr\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","import nltk\n","from nltk.corpus import stopwords\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","from nltk.tokenize import word_tokenize\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","text = word_tokenize(DataInStr)\n","data = [word for word in text if not word in stopwords.words()]\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","print(data)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","data\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","from nltk.stem import WordNetLemmatizer\n","nltk.download('wordnet')\n","lemma = WordNetLemmatizer()\n","lem = []\n","for r in data:\n","    lem.append(lemma.lemmatize(r))\n","print(lemma.lemmatize('knows'))\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","import re\n","dataupdate = []\n","dataupdate = [re.sub('[^a-zA-z0-9]+','',_) for _ in lem]\n","dataupdate\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.cluster import KMeans\n","from sklearn.metrics import adjusted_rand_score\n","# import warnings filter\n","from warnings import simplefilter\n","simplefilter(action='ignore',category=FutureWarning)\n","\n","vectorizer = TfidfVectorizer(stop_words='english')\n","x = vectorizer.fit_transform(dataupdate)\n","\n","true_k = 8\n","model = KMeans(n_clusters=true_k,init='k-means++',max_iter=100,n_init=1)\n","model.fit(x)\n","\n","print(\"Top term per cluster\")\n","order_centroids = model.cluster_centers_.argsort()[:,::-1]\n","\n","terms = vectorizer.get_feature_names()\n","for  i in range(true_k):\n","    print(\"Cluster %id\"%i),\n","    for ind in order_centroids[i, :10]:\n","        print('%s '%terms[ind]),\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","\n","\n"]}],"metadata":{"interpreter":{"hash":"7ae0aef52bef687a3c48202339f3a6d3f7667471f22019098c9e93c6041c6fc2"},"kernelspec":{"display_name":"Python 3.9.7 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
